{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "#import pymongo\n",
    "#import random\n",
    "import csv\n",
    "import time\n",
    "import re\n",
    "import matplotlib.pyplot as plt; import matplotlib.pylab as pylab\n",
    "#%matplotlib inline\n",
    "pd.options.display.mpl_style = 'default'\n",
    "pylab.rcParams['figure.figsize'] = 12, 6\n",
    "from dateutil import parser\n",
    "#import Quandl\n",
    "#from pymongo import MongoClient\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Get A BeautifulSoup Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BloombergSearch:\n",
    "    def __init__(self, search_term):\n",
    "        self.search_term = search_term\n",
    "        self.url_page1 = ('http://www.bloomberg.com/search?query=' + str(self.search_term))\n",
    "\n",
    "    def get_search_soup(self):\n",
    "        url =  self.url_page1\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "    def get_soup(self, url):\n",
    "        page = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        return soup\n",
    "    def get_search_page_links(self, num_pages):\n",
    "        article_list = []\n",
    "        for i in range(1, num_pages + 1):\n",
    "            temp_soup = self.get_soup(self.url_page1 + str('&page=') + str(i))\n",
    "            for result in temp_soup.find_all('h1'):\n",
    "                try:\n",
    "                    if 'video' in result.a['href']:\n",
    "                        continue\n",
    "                    if 'http' in result.a['href']:\n",
    "                        #print item.a['href']\n",
    "                        article_list.append(result.a['href'])\n",
    "                    else:\n",
    "                        #print 'http://www.bloomberg.com/' + item.a['href']\n",
    "                        article_list.append('http://www.bloomberg.com/' + result.a['href'])\n",
    "                        print('http://www.bloomberg.com/' + result.a['href'])\n",
    "                except:\n",
    "                    continue\n",
    "            #print 'Added page=' + str(i)\n",
    "        return article_list\n",
    "    \n",
    "    def get_post_body(self, article_url):\n",
    "        final_text = \"\"\n",
    "        article_soup = self.get_soup(article_url)\n",
    "        query = article_soup.find_all('div',  class_=\"article-body__content\")\n",
    "        for item in query:\n",
    "            for text in item.find_all('p'):\n",
    "                final_text = final_text + '\\n\\n' + str(text.text.encode('utf-8'))\n",
    "        if final_text == \"\":\n",
    "            return 0\n",
    "        return final_text\n",
    "    \n",
    "    def get_post_date(self, article_url):\n",
    "        final_text = \"\"\n",
    "        article_soup = self.get_soup(article_url)\n",
    "        result = article_soup.find('time', class_ = \"published-at\")\n",
    "        try:\n",
    "            return result['datetime']\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_post_author(self, article_url):\n",
    "        final_text = \"\"\n",
    "        article_soup = self.get_soup(article_url)\n",
    "        result = article_soup.find('a', class_ = \"author-link\")\n",
    "        try:\n",
    "            return result.text.lstrip().rstrip()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def get_post_title(self, article_url):\n",
    "        final_text = \"\"\n",
    "        article_soup = self.get_soup(article_url)        \n",
    "        result = article_soup.find('title')\n",
    "        try:\n",
    "            return result.text.lstrip().rstrip()\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    def make_info(self, pages = 1):\n",
    "        final_df = pd.DataFrame()\n",
    "        for url in self.get_search_page_links(pages):\n",
    "            body = self.get_post_body(url)\n",
    "            title = self.get_post_title(url)\n",
    "            author = self.get_post_author(url)\n",
    "            date = self.get_post_date(url)\n",
    "            temp_series = pd.Series([title, author, date, body])\n",
    "            final_df= final_df.append(temp_series, ignore_index = True)\n",
    "        final_df.columns = ['title', 'author', 'date', 'text']\n",
    "        return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bloom_obj = BloombergSearch('ibm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.bloomberg.com/news/articles/2016-02-27/buffett-reiterates-praise-for-ibm-amex-leaders-amid-share-slump\n",
      "http://www.bloomberg.com/news/articles/2016-02-25/ibm-changes-reporting-segments-in-a-move-toward-transparency\n",
      "http://www.bloomberg.com/news/articles/2016-02-25/ims-health-pays-its-ceo-more-than-ibm-s-to-keep-him-from-leaving\n",
      "http://www.bloomberg.com/news/articles/2016-02-29/sec-insider-trading-case-testing-new-standards-goes-to-jury\n",
      "http://www.bloomberg.com/news/articles/2016-02-24/ibm-paid-1-3-billion-to-acquire-cleversafe-in-hybrid-cloud-push\n",
      "http://www.bloomberg.com/news/articles/2016-02-28/buffett-hits-negative-drumbeat-clinton-wins-sc-saturday-wrap\n",
      "http://www.bloomberg.com/news/articles/2016-02-26/buffett-s-empty-calendar-offers-billionaire-path-to-bigger-ideas\n",
      "http://www.bloomberg.com/news/articles/2016-02-18/ibm-to-acquire-truven-health-analytics-for-2-6-billion-iksc5n0r\n",
      "http://www.bloomberg.com/news/articles/2016-02-18/ibm-to-acquire-truven-health-analytics-for-2-6-billion\n",
      "http://www.bloomberg.com/news/articles/2016-02-16/blockchain-coming-to-japan-s-markets-as-jpx-teams-up-with-ibm\n",
      "http://www.bloomberg.com/news/articles/2016-02-18/dentsu-considers-as-many-as-60-m-a-deals-amid-push-for-ad-talent\n",
      "http://www.bloomberg.com/news/articles/2016-02-18/how-an-expensive-suit-can-make-you-better-at-your-job\n",
      "http://www.bloomberg.com/news/articles/2016-02-18/u-s-futures-little-changed-after-best-s-p-500-jump-since-august\n",
      "http://www.bloomberg.com/news/articles/2016-02-16/the-german-coal-miner-using-1-000-ipads-to-help-weather-a-rout\n",
      "http://www.bloomberg.com/news/articles/2016-02-16/apple-said-to-plan-dollar-bond-sale-for-capital-return-program\n",
      "http://www.bloomberg.com/news/articles/2016-02-16/corporate-bond-deals-take-off-with-apple-and-ibm-tapping-market\n",
      "http://www.bloomberg.com/news/articles/2016-02-16/treasuries-recover-loss-as-oil-deal-seen-lacking-shock-and-awe\n",
      "http://www.bloomberg.com/news/articles/2016-02-10/munger-says-jury-is-out-on-whether-ibm-s-reinvention-will-work\n",
      "http://www.bloomberg.com/news/articles/2016-02-09/coned-taps-ibm-to-supply-new-yorkers-with-real-time-power-data\n",
      "http://www.bloomberg.com/news/articles/2016-02-02/goldman-sachs-and-ibm-join-investment-in-digital-asset-holdings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OMalley/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'get_post_body'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0b8539f4d77c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbloom_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtemp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_post_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/OMalley/anaconda3/lib/python3.5/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2358\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m             raise AttributeError(\"'%s' object has no attribute '%s'\" %\n\u001b[0;32m-> 2360\u001b[0;31m                                  (type(self).__name__, name))\n\u001b[0m\u001b[1;32m   2361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'get_post_body'"
     ]
    }
   ],
   "source": [
    "temp = bloom_obj.make_info(3)\n",
    "temp.get_post_body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ForbesSearch:\n",
    "    def __init__(self, search_term):\n",
    "        self.search_term = search_term\n",
    "        self.url_page1 = ('http://www.forbes.com/search/?q=' + str(self.search_term))\n",
    "\n",
    "    def get_search_soup(self):\n",
    "        url =  self.url_page1\n",
    "        soup = self.get_soup(url)\n",
    "        return soup\n",
    "    def get_soup(self, url):\n",
    "        page = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(page)\n",
    "        return soup\n",
    "    def get_search_page_links(self, num_pages):\n",
    "        article_list = []\n",
    "        for i in range(1, num_pages + 1):\n",
    "            temp_soup = self.get_soup(self.url_page1 + str('&page=') + str(i))\n",
    "            for result in temp_soup.find_all('h2'):\n",
    "                try:\n",
    "                    if 'video' in result.a['href']:\n",
    "                        continue\n",
    "                    if 'http' in result.a['href']:\n",
    "                        #print item.a['href']\n",
    "                        article_list.append(result.a['href'])\n",
    "                    else:\n",
    "                        #print 'http://www.bloomberg.com/' + item.a['href']\n",
    "                        article_list.append('http://www.forbes.com/' + result.a['href'])\n",
    "                except:\n",
    "                    continue\n",
    "            #print 'Added page=' + str(i)\n",
    "        return article_list\n",
    "    \n",
    "    def get_post_body(self, article_url):\n",
    "        final_text = \"\"\n",
    "        article_soup = self.get_soup(article_url)\n",
    "        query = article_soup.find_all('script')\n",
    "        item = query[1]\n",
    "       # final_text = str(item.text).encode('utf-8')\n",
    "        indexA = item.text.find('\"body\":\"<p>') \n",
    "        indexB = item.text.find('</p>\",\"description\"')\n",
    "        \n",
    "        pointer = indexA + 11\n",
    "        while (pointer != indexB):\n",
    "            if (item.text[pointer] == \"<a>\" or item.text[pointer] == \"<\"):\n",
    "                while (item.text[pointer] != \"</a>\" and item.text[pointer] != \">\"):\n",
    "                    pointer = pointer + 1\n",
    "            final_text = final_text + str((item.text[pointer]))\n",
    "            pointer = pointer + 1\n",
    "        final_text = final_text.replace('\\\\r\\\\n\\\\r\\\\n', '')\n",
    "        final_text = final_text.replace('\\\\', '')\n",
    "        return final_text\n",
    "    \n",
    "    def make_info(self, pages = 1):\n",
    "        for url in self.get_search_page_links(pages):\n",
    "            body = self.get_post_body(url)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bloom_obj = ForbesSearch('ibm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/OMalley/anaconda3/lib/python3.5/site-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[caption id=\"attachment_490899468\" align=\"\" width=\"960\"]> Chief Executive Officer and President of The AES Corporation Andres Gluski . (Photo by Leigh Vogel/Getty Images for Concordia Summit)[/caption]Earlier this month The AES Corporation >announced> partnerships designed to sell&nbsp;its Advancion utility-scale battery system&nbsp;around the globe. But AES did not include income&nbsp;from those partnerships in the 2016&nbsp;outlook it released today.AES CEO Andres Gluski nonetheless expects the battery storage market to be \"quite large\" in five years&mdash;not quite as enthusiastic an assessment as Tesla Motors CEO Elon Musk\\'s description of the market as \">staggeringly gigantic>.\"\"In our current financial guidance, we do not include any material income from third-party sales of our Advancion project. We believe this is prudent, since our alliance-based business model is in its initial phases,\" Gluski said in a conference call with analysts this morning.Gluski reported lower earnings than last year but still&nbsp;beat estimates by a penny, and AES stock was up about 4 percent this afternoon in the wake of the call.Using batteries manufactured by LG Chem, AES will partner with Mitsubishi to sell Advancion in Asia and Australia and with Eaton Corporation in Europe, the Middle East and Africa.Tesla has begun selling Powerwalls and Powerpacks in Australia and Germany, Musk said earlier this month, and has opened an office in South Africa. Musk predicted gross margins from battery sales would grow steadily this year. In a prior conference call, he predicted up to $500 million in sales in 2016.Gluski believes demand for utility-scale batteries will be driven by renewables:rn>rn\"We believe that the global market for energy storage solutions is expanding rapidly and will be quite large within five years,\" he said, \"as utilities respond to greater renewable penetration.\"Musk believes the market for utility-scale storage is staggeringly gigantic even without renewables, because once storage is cheap enough, it allows utilities to close power plants or defer new plants.\"It seems like people link this too much to renewable energy,&rdquo; he&nbsp;said last summer. &ldquo;You can basically, in principle, shut down half of the world&rsquo;s power plants if you had stationary storage.&rdquo;Tesla\\'s Nevada Gigafactory began churning out Powerpacks and Powerwalls&nbsp;at the end of 2015, but AES\\'s Battery Integration Center in Indianapolis has been combining LG Chem batteries with other people\\'s inverters and its own control equipment and software since early 2014.As a result, AES has&nbsp;106 megawatts of energy storage in&nbsp;operation in four countries. \"We have another 60 megawatts under construction and a further 228 megawatts in advanced stage development in the U.S., Latin America and Asia, including 100 megawatts under contract in California,\" Gluski said.The big difference going forward&nbsp;may be&nbsp;cost. Tesla sells Powerpacks for $250/kWh of capacity, well below a $350 threshold that serves as&nbsp;a marker >akin to&nbsp;grid parity> in some regions. AES has been more cagey about cost, >telling> CleanTechnica, \"System costs vary depending on the size, duration, and online date of the project.\"'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = bloom_obj.get_post_body('http://www.forbes.com/sites/jeffmcmahon/2016/02/24/aes-teslas-battery-competitor-hedges-its-bets-on-energy-storage/print/')\n",
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
